\chapter{Speech Processing}

\label{SpeechProcessing}

The speech-processing component mainly comprises a microservice wrapping OpenAIs Whisper project and the management 
that utilizes this component to process audio streams. It receives slices of the audio stream, various in length, and
transcribes them into text.

Since Whisper cannot process audio streams directly but solely complete audio files, this service is required to build 
the basis to transcribe slices of the audio stream.

%----------------------------------------------------------------------------------------

\section{Microservice Wrapper}

The speech-processing pipeline contains a microservice for processing audio streams with OpenAIs Whisper. It receives 
transcription requests via an HTTP API and returns the resulting JSON information about the audio file.

\subsection{Faster Whisper}

Since the system's hardware uses Nvidia graphics cards, the microservice uses a GPU-accelerated version of Whisper 
called Faster Whisper. It brings CUDA support to allow for GPU-accelerated processing of audio files when dealing with 
Nvidia graphics cards. The microservice checks for the availability of CUDA support on startup and falls back to the 
CPU version if it is unavailable.

Since the interface for Faster Whisper is only available in the Python programming language, but the interfacing 
service is preferred to be developed in Node.js, the Whisper access is implemented by building a CLI interface that 
calls all necessary operations of Faster Whisper. This design decision allows the developer to test the whisper 
interface independently since it can run as a terminal application.

The Node.js interface for the microservice itself spawns a child process that runs this application and communicates 
with it over the standard input/output.

\subsection{Service Pool}

The versatility of this design pattern made another improvement very easy to implement. Since the capabilities of 
different graphics cards are various, and one audio file processing job might not utilize the full potential of the 
hardware in question, it makes sense to provide a service pool with a configurable amount of transcription workers 
available. One transcription worker would be one instance of the previously explained CLI tool.

Due to the chosen design, this improvement is natural by holding a ServicePool with numerous worker instances. When 
requesting a job, the pool uses the next available worker instance.

\subsection{Interface}

The HTTP interface of the microservice is straightforward. It provides a single endpoint for transcribing audio files 
and returns the resulting JSON information about the audio file. The endpoint is available under the root path of the 
microservice and expects a POST request with the audio file as the body of the request. The audio file has to be in a 
WAV format and encoded as base64. There is also an optional query parameter to specify the language of the audio 
content. The microservice returns the resulting JSON information about the audio file.

%----------------------------------------------------------------------------------------

\section{Language Detection}

If Whisper has no information about the language of the audio file, it determines a list of possible languages. The 
audio file transcription gets generated in the language with the highest probability. The speech-processing pipeline 
also uses this integrated language detection to determine the language of the audio stream.

Since this is not always accurate, especially with short audio files, the speech-processing pipeline brings additional 
safeguards to reduce the chance of misinterpreted audio files.

Suppose there is no previously known language for the user that sends the audio stream. In that case, the transcription 
microservice will not get any assumption about the language and, therefore, determine it by itself. This language will 
be returned and saved as the user's language for future audio streams.

Since this assumption can be false, or the user might switch to another language, the speech-processing pipeline 
assumes the previous language as long as a message has less than five words. From this point on, language detection 
will reactivate again to determine the language of the audio stream. This safeguard reduces the chance of 
misinterpreted audio files since the user will most likely not switch the language of the audio stream in the middle 
of the conversation. Concise messages are the most vulnerable to language detection errors, and this protection 
prevents these.


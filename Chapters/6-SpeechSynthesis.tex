\chapter{Speech Synthesis and Audio Data Transmission}

\label{SpeechSynthesisAndAudioDataTransmission}

Speech synthesis and transmitting the resulting audio data to the proper users is crucial for the System to work. The 
following components are responsible for this task. Especially for users only connected via Voice over IP, this is the 
only way to provide them with the translated message.

%----------------------------------------------------------------------------------------

\section{Speech Synthesis}

The System uses PiperTTS, an open-source text-to-speech library, to synthesize spoken audio from text. It supports 
multiple languages and voices and allows the System to create audio files in any language with a voice model.

The System uses PiperTTS to create audio files from the translated messages. The audio files are stored in the 
filesystem and available via a unique ID. The ID gets returned to the session handling component, which uses it to send 
the audio file to the appropriate users.

\subsection{TTS Microservice}

Like the transcription microservice, the speech synthesis microservice wraps the PiperTTS CLI interface and 
provides a convenient HTTP API interface to the rest of the System. The service spawns a child process that runs the 
PiperTTS CLI application and awaits the generation of the audio file. It then returns the audio file as an HTTP response 
encoded as audio/wav.

The HTTP interface of the microservice is straightforward. It provides a single endpoint for synthesizing audio files 
and returns the resulting audio file. The endpoint is available under the root path of the microservice and expects a 
POST request with the text to synthesize as the body of the request. There is one required query parameter to specify 
the language of the audio content. Additionally, there are optional query parameters to specify the preferred country, 
gender, or a specific voice model. The microservice returns the resulting audio file.

\begin{verbatim}
POST / HTTP/1.1
Content-Type: text/plain
Content-Length: 28
Accept: audio/wav

Hello World, this is a test.

\end{verbatim}

\subsection{Audio Generation}

Since PiperTTS utilizes the entire CPU by splitting the audio generation into as many threads as there are CPU cores, 
the System can utilize the hardware's full potential without creating a service pool like the transcription 
microservice.

The generated audio file is in the WAV format and encoded as audio/wav. The System stores the audio file in the 
filesystem. It returns the ID of the audio file to the session handling componentâ€”the session handling component uses 
the ID to send the audio file to the appropriate users. 

%----------------------------------------------------------------------------------------

\section{Audio Data Transmission}

After the audio file is available, the System needs to send it to the appropriate users. This procedure is different 
depending on the audio input channel. The System uses WebSockets to send the audio file to users connected via a web 
browser. For users connected via Voice over IP, the System uses a UDP backchannel to send the audio file to the VoIP 
provider.

\subsection{Web Browser Audio}

If the user connects via a browser audio stream, the System sends a WebSocket message to the client to notify it about 
the existence of a newly synthesized file. The web client uses the transmitted ID property to download the audio WAV 
file and uses the Web Audio API to play it back.

The WebSocket message contains the ID of the audio file as well as data about the nature of the synthesized audio file. 
The web client uses this information to determine the correct way to play back the audio file. The System sends the 
following WebSocket message to the client.

\begin{verbatim}
{
    "type": "tts",
    "audioId": "1c4e0c77-f3d9-5ba5-bc88-c2177926514d",
    "playbackType": "translation"
}
\end{verbatim}

The "type" property allows the use of one single WebSocket connection for all kinds of messages and helps the client 
determine the correct way to handle the message. In this case, it is a text-to-speech event. The "audioId" property 
contains the ID of the audio file. The "playbackType" property contains the type of the audio file. In this case, it is 
a translation of a message sent by another user. It can also be assigned to the value "translation-playback." The 
"translation-playback" value is used to play back the translation of the user's message.

With this design decision, the User Interface can easily support features to play back the translations in different 
volume levels or even allow client-side configuration to turn off the playback of specific translations altogether.

\subsection{Voice over IP Audio}

If the user connects via a VoIP provider, the System sends a UDP message to the provider via a previously configured 
backchannel port. The message contains the ID of the audio file, and the VoIP provider uses the ID to download the 
audio WAV file and plays it back into the audio stream of the phone call. 

The UDP message contains only the ID of the audio file. The VoIP provider uses the ID to download the audio file and 
play it back to the user. The System sends the following UDP message to the VoIP provider. 

\begin{verbatim}
WaveFile:30ea2c43-bbdc-5db6-8d46-78ddbc2b1b72
\end{verbatim}

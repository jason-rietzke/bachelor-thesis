\chapter{Conclusion}

\label{Conclusion}

This chapter summarizes the System's capabilities and compares them to the requirements defined in Chapter 
\ref{Introduction}. It also discusses the System's limitations and potential future work.

%----------------------------------------------------------------------------------------

\section{Summary}

This thesis aimed to develop a System that allows users to communicate with each other in foreign languages in near 
real-time. The System should be able to handle multiple audio streams simultaneously and detect the spoken language 
automatically. It should also be able to translate the audio content into the user's preferred language and provide the 
translated text and audio to the user. Furthermore, the System should be built using open-source software and be able 
to run on commodity hardware.

The System developed in this thesis can handle audio streams from web browsers and provides an interface for Voice over 
IP providers to send a UDP audio stream to be processed, aiming to integrate with VoIP providers. The pipeline takes 
the various audio sources, transforms them internally into the same format, and handles them the same way from that 
point on.

The near real-time transcription of the audio streams is achieved by utilizing the open-source OpenAI Whisper project. 
The System enhances Whisper's capabilities by enabling it to transcribe audio streams instead of final audio files. 
This reaches the goal of streaming support and near real-time transcription.

Automatic language detection is achieved by utilizing the same Whisper service. It has a very capable language 
detection feature that can detect the spoken language of an audio stream with high accuracy.

The System uses the open-source PiperTTS project to synthesize spoken audio from text. It allows the System to 
synthesize audio files with various voices in different languages. The System uses PiperTTS to synthesize audio files 
from the translated messages and stores them in the filesystem. Implementing this feature reaches the goal of speech 
synthesis based on open-source software.

The only external proprietary component the speech-processing pipeline uses is the DeepL translation API to translate 
the transcription results into the required languages within a session. This is the only component that is not 
open-source since the quality and versatility of open-source translation services do not meet expectations.

%----------------------------------------------------------------------------------------

\section{Potential Future Work}
Even though the developed speech-processing pipeline meets most requirements, there is still room for improvement. 
The following sections discuss potential future work.

\subsection{Open-Source Translation Service}

The System uses the DeepL translation service to translate the transcription results into the required languages. 
DeepL is a commercial translation service that provides a high-quality translation API. However, the versatility of the 
System design allows for the future replacement of the DeepL translation service with an open-source solution. 

Since new AI models are continuously being developed, open-source translation services may be available in the future. 
Integrating such a service into the System would be straightforward since the System already provides an interface for 
translation services.

\subsection{OpenAI Whisper Models}

Throughout this thesis, OpenAI released an improved version of the Whisper model. The new model is called "large-v3" 
and improves accuracy even further. The Faster-Whisper project, used in the pipeline, has yet to support the new model. 
However, it is planned to add support for the improved model soon.

Furthermore, it is reasonable to assume that OpenAI will release even more improved models over time. It is advisable 
to keep an eye on the development of the Whisper project and update the System accordingly.

\subsection{Noise Reduction}

OpenAI Whisper does a decent job of filtering out background noise and only transcribing the spoken words. However, 
there is still room for improvement. The System could use a noise reduction and voice activation algorithm to filter 
out background noise further and only request a transcription if a human voice is recognized. This would improve the 
transcription accuracy even more and prevent the System from transcribing background noises.
